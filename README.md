# <center> Awesome-Large-Audio-Models
## ‚ú®‚ú® Awesome-Large-Audio-Models ‚ú®‚ú®

Welcome to Awesome-Large-Audio-Models, your go-to resource for the latest advancements, papers, and benchmarks in the world of cutting-edge audio models. This repository is meticulously curated for researchers, developers, and enthusiasts passionate about pushing the boundaries of audio processing and understanding.

### üìö Latest Papers

Stay at the forefront of audio model research with these recent papers:

- **"AudioPaLM: A Large Language Model That Can Speak and Listen"**
    - *Authors*: *Paul K. Rubenstein, Chulayuth Asawaroengchai, Duc Dung Nguyen, Ankur Bapna, Zal√°n Borsos, F√©lix de Chaumont Quitry, Peter Chen, Dalia El Badawy, Wei Han, Eugene Kharitonov, Hannah Muckenhirn, Dirk Padfield, James Qin, Danny Rozenberg, Tara Sainath, Johan Schalkwyk, Matt Sharifi, Michelle Tadmor Ramanovich, Marco Tagliasacchi, Alexandru Tudor, Mihajlo Velimiroviƒá, Damien Vincent, Jiahui Yu, Yongqiang Wang, Vicky Zayats, Neil Zeghidour, Yu Zhang, Zhishuai Zhang, Lukas Zilka, Christian Frank*
    - *[PDF](https://arxiv.org/abs/2306.12925)*
- **"Speechgpt: Empowering large language models with intrinsic cross-modal conversational abilities"**
    - *Authors*: *Dong Zhang, Shimin Li, Xin Zhang, Jun Zhan, Pengyu Wang, Yaqian Zhou, Xipeng Qiu
    - *[PDF](https://arxiv.org/abs/2305.11000) [Code](https://github.com/0nutation/SpeechGPT)*
- **"SeamlessM4T‚ÄîMassively Multilingual & Multimodal Machine Translation"**
    - *Authors*: *Seamless Communication, Lo√Øc Barrault, Yu-An Chung, Mariano Cora Meglioli, David Dale, Ning Dong, Paul-Ambroise Duquenne, Hady Elsahar, Hongyu Gong, Kevin Heffernan, John Hoffman, Christopher Klaiber, Pengwei Li, Daniel Licht, Jean Maillard, Alice Rakotoarison, Kaushik Ram Sadagopan, Guillaume Wenzek, Ethan Ye, Bapi Akula, Peng-Jen Chen, Naji El Hachem, Brian Ellis, Gabriel Mejia Gonzalez, Justin Haaheim, Prangthip Hansanti, Russ Howes, Bernie Huang, Min-Jae Hwang, Hirofumi Inaguma, Somya Jain, Elahe Kalbassi, Amanda Kallet, Ilia Kulikov, Janice Lam, Daniel Li, Xutai Ma, Ruslan Mavlyutov, Benjamin Peloquin, Mohamed Ramadan, Abinesh Ramakrishnan, Anna Sun, Kevin Tran, Tuan Tran, Igor Tufanov, Vish Vogeti, Carleigh Wood, Yilin Yang, Bokai Yu, Pierre Andrews, Can Balioglu, Marta R. Costa-juss√†, Onur Celebi, Maha Elbayad, Cynthia Gao, Francisco Guzm√°n, Justine Kao, Ann Lee, Alexandre Mourachko, Juan Pino, Sravya Popuri, Christophe Ropers, Safiyyah Saleem, Holger Schwenk, Paden Tomasello, Changhan Wang, Jeff Wang, Skyler Wang*
    - *[PDF](https://arxiv.org/abs/2308.11596) [Code](https://github.com/facebookresearch/seamless_communication)*
- ‚Äú**PolyVoice: Language Models for Speech to Speech Translation**‚Äù
    - *Authors*: *Qianqian Dong, Zhiying Huang, Qiao Tian, Chen Xu, Tom Ko, Yunlong Zhao, Siyuan Feng, Tang Li, Kexin Wang, Xuxin Cheng, Fengpeng Yue, Ye Bai, Xi Chen, Lu Lu, Zejun Ma, Yuping Wang, Mingxuan Wang, Yuxuan Wang*
    - *[PDF](https://arxiv.org/abs/2306.02982)*
- **"Unified Model for Image, Video, Audio and Language Tasks"**
    - *Authors*: *Mustafa Shukor, Corentin Dancette, Alexandre Rame, Matthieu Cord*
    - *[PDF](https://arxiv.org/abs/2307.16184) [Code](https://github.com/mshukor/UnIVAL)*
- **‚ÄúTokenSplit: Using Discrete Speech Representations for Direct, Refined, and Transcript-Conditioned Speech Separation and Recognition‚Äù**
    - *Authors*: *Hakan Erdogan, Scott Wisdom, Xuankai Chang, Zal√°n Borsos, Marco Tagliasacchi, Neil Zeghidour, John R. Hershey*
    - *[PDF](https://arxiv.org/abs/2308.10415)*
- **‚ÄúPrompting and Adapter Tuning for Self-supervised Encoder-Decoder Speech Model‚Äù**
    - *Authors*: *Kai-Wei Chang, Ming-Hsin Chen, Yun-Ping Lin, Jing Neng Hsu, Paul Kuo-Ming Huang, Chien-yu Huang, Shang-Wen Li, Hung-yi Lee*
    - *[PDF](https://arxiv.org/abs/2310.02971)*
- **‚ÄúMultilingual Speech-to-Speech Translation into Multiple Target Languages‚Äù**
    - *Authors*: *Hongyu Gong, Ning Dong, Sravya Popuri, Vedanuj Goswami, Ann Lee, Juan Pino*
    - *[PDF](https://arxiv.org/abs/2307.08655)*
- ‚Äú**Textless Direct Speech-to-Speech Translation with Discrete Speech Representation**‚Äù
    - *Authors*: *Xinjian Li, Ye Jia, Chung-Cheng Chiu*
    - *[PDF](https://ieeexplore.ieee.org/abstract/document/10096797)*
- ‚Äú**SpeechGen: Unlocking the Generative Power of Speech Language Models with Prompts**‚Äù
    - *Authors: Haibin Wu, Kai-Wei Chang, Yuan-Kuei Wu, Hung-yi Lee*
    - [PDF](https://arxiv.org/abs/2306.02207) [Code](https://ga642381.github.io/SpeechPrompt/speechgen)
- **‚ÄúListen, Think, and Understand‚Äù**
    - *Authors: Yuan Gong, Hongyin Luo, Alexander H. Liu, Leonid Karlinsky, James Glass*
    - [PDF](https://arxiv.org/abs/2305.10790)
- ‚Äú**BuboGPT: Enabling Visual Grounding in Multi-Modal LLMs‚Äù**
    - *Authors: Yang Zhao, Zhijie Lin, Daquan Zhou, Zilong Huang, Jiashi Feng, Bingyi Kang*
    - [PDF](https://arxiv.org/abs/2307.08581)

### üî• Speech to Text

Stay up-to-date with the latest advancements in speech-to-text models:

- **"Robust Speech Recognition via Large-Scale Weak Supervision"**
    - *Authors*: *Alec Radford, Jong Wook Kim, Tao Xu, Greg Brockman, Christine McLeavey, Ilya Sutskever*
    - *[PDF](https://arxiv.org/abs/2212.04356) [Code](https://github.com/openai/whisper)*
- **‚ÄúWhisperX: Time-Accurate Speech Transcription of Long-Form Audio‚Äù**
    - *Authors: Max Bain, Jaesung Huh, Tengda Han, Andrew Zisserman*
    - *[PDF](https://arxiv.org/abs/2303.00747) [Code](https://github.com/m-bain/whisperX)*
- **"Google USM: Scaling Automatic Speech Recognition Beyond 100 Languages"**
    - *Authors*: *Yu Zhang, Wei Han, James Qin, Yongqiang Wang, Ankur Bapna, Zhehuai Chen, Nanxin Chen, Bo Li, Vera Axelrod, Gary Wang, Zhong Meng, Ke Hu, Andrew Rosenberg, Rohit Prabhavalkar, Daniel S. Park, Parisa Haghani, Jason Riesa, Ginger Perng, Hagen Soltau, Trevor Strohman, Bhuvana Ramabhadran, Tara Sainath, Pedro Moreno, Chung-Cheng Chiu, Johan Schalkwyk, Fran√ßoise Beaufays, Yonghui Wu*
    - *[PDF](https://arxiv.org/abs/2303.01037)*
- **‚ÄúPrompting Large Language Models for Zero-Shot Domain Adaptation in Speech Recognition‚Äù**
    - *Authors*: *Yuang Li, Yu Wu, Jinyu Li, Shujie Liu*
    - *[PDF](https://arxiv.org/abs/2306.16007)*
- **‚ÄúExploring the Integration of Large Language Models into Automatic Speech Recognition Systems: An Empirical Study‚Äù**
    - *Authors*: *Zeping Min, Jinbo Wang*
    - *[PDF](https://arxiv.org/abs/2307.06530)*
- **‚ÄúOn decoder-only architecture for speech-to-text and large language model integration‚Äù**
    - *Authors*: *Jian Wu, Yashesh Gaur, Zhuo Chen, Long Zhou, Yimeng Zhu, Tianrui Wang, Jinyu Li, Shujie Liu, Bo Ren, Linquan Liu, Yu Wu*
    - *[PDF](https://arxiv.org/abs/2307.03917)*
- **‚ÄúAdapting Large Language Model with Speech for Fully Formatted End-to-End Speech Recognition‚Äù**
    - *Authors*: *Shaoshi Ling, Yuxuan Hu, Shuangbei Qian, Guoli Ye, Yao Qian, Yifan Gong, Ed Lin, Michael Zeng*
    - *[PDF](https://arxiv.org/abs/2307.08234)*
- ‚Äú**Adapting multilingual speech representation model for a new, underresourced 
language through multilingual fine-tuning and continued pretraining‚Äù**
    - *Authors*: *Karol Nowakowski, Michal Ptaszynski, Kyoko Murasaki, Jagna Nieuwa≈ºny*
    - *[PDF](https://www.sciencedirect.com/science/article/abs/pii/S0306457322002497)*
- ‚Äú**Prompting Large Language Models with Speech Recognition Abilities**‚Äù
    - *Authors*: *Yassir Fathullah, Chunyang Wu, Egor Lakomkin, Junteng Jia, Yuan Shangguan, Ke Li, Jinxi Guo, Wenhan Xiong, Jay Mahadeokar, Ozlem Kalinli, Christian Fuegen, Mike Seltzer*
    - *[PDF](https://arxiv.org/abs/2307.11795)*
- **‚ÄúSemantic Segmentation with Bidirectional Language Models Improves Long-form ASR‚Äù**
    - *Authors*: *W. Ronny Huang, Hao Zhang, Shankar Kumar, Shuo-yiin Chang, Tara N. Sainath*
    - *[PDF](https://arxiv.org/abs/2305.18419)*
- **‚ÄúFLEURS: FEW-Shot Learning Evaluation of Universal Representations of Speech‚Äù**
    - *Authors: Alexis Conneau, Min Ma, Simran Khanuja, Yu Zhang, Vera Axelrod, Siddharth Dalmia, Jason Riesa, Clara Rivera, Ankur Bapna*
    - *[PDF](https://ieeexplore.ieee.org/abstract/document/10023141)*
- **‚ÄúMASR: Multi-label Aware Speech Representation‚Äù**
    - *Authors: Anjali Raj, Shikhar Bharadwaj, Sriram Ganapathy, Min Ma, Shikhar Vashishth*
    - *[PDF](https://arxiv.org/abs/2307.10982)*
- ‚Äú**Exploring Speech Recognition, Translation, and Understanding with Discrete Speech Units: A Comparative Study**‚Äù
    - *Authors: Xuankai Chang, Brian Yan, Kwanghee Choi, Jeeweon Jung, Yichen Lu, Soumi Maiti, Roshan Sharma, Jiatong Shi, Jinchuan Tian, Shinji Watanabe, Yuya Fujita, Takashi Maekaku, Pengcheng Guo, Yao-Fei Cheng, Pavel Denisov, Kohei Saijo, Hsiu-Hsuan Wang*
    - *[PDF](https://arxiv.org/abs/2309.15800)*
- **‚ÄúSpeechTokenizer: Unified Speech Tokenizer for Speech Large Language Models‚Äù**
    - *Authors: Xin Zhang, Dong Zhang, Shimin Li, Yaqian Zhou, Xipeng Qiu*
    - *[PDF](https://arxiv.org/abs/2308.16692)*

### üéß Audio Generation

Stay up to date on the latest advances in audio generation models:

- **‚ÄúAudioLM: A Language Modeling Approach to Audio Generation‚Äù**
    - *Authors: Zal√°n Borsos, Rapha√´l Marinier, Damien Vincent, Eugene Kharitonov, Olivier Pietquin, Matt Sharifi,Dominik Roblek, Olivier Teboul, David Grangier, Marco Tagliasacchi, and Neil Zeghidour*
    - *[PDF](https://ieeexplore.ieee.org/abstract/document/10158503)*
- ‚Äú**AudioGen: Textually Guided Audio Generation‚Äù**
    - *Authors: Felix Kreuk, Gabriel Synnaeve, Adam Polyak, Uriel Singer, Alexandre D√©fossez, Jade Copet, Devi Parikh, Yaniv Taigman, Yossi Adi*
    - *[PDF](https://arxiv.org/abs/2209.15352)*
- **‚ÄúFoundationTTS: Text-to-Speech for ASR Customization with Generative Language Model‚Äù**
    - *Authors: Ruiqing Xue, Yanqing Liu, Lei He, Xu Tan, Linquan Liu, Edward Lin, Sheng Zhao*
    - *[PDF](https://arxiv.org/abs/2303.02939)*
- **‚ÄúText-to-Audio Generation using Instruction-Tuned LLM and Latent Diffusion Model‚Äù**
    - *Authors: Deepanway Ghosal, Navonil Majumder, Ambuj Mehrish, Soujanya Poria*
    - *[PDF](https://arxiv.org/abs/2304.13731) [Code](https://github.com/declare-lab/tango)*
- **‚ÄúWavCaps: A ChatGPT-Assisted Weakly-Labelled Audio Captioning Dataset for Audio-Language Multimodal Research‚Äù**
    - *Authors: Xinhao Mei, Chutong Meng, Haohe Liu, Qiuqiang Kong, Tom Ko, Chengqi Zhao, Mark D. Plumbley, Yuexian Zou, Wenwu Wang*
    - *[PDF](https://arxiv.org/abs/2303.17395) [Code](https://github.com/XinhaoMei/WavCaps)*
- **‚ÄúAudioLDM: Text-to-Audio Generation with Latent Diffusion Models‚Äù**
    - *Authors: Haohe Liu, Zehua Chen, Yi Yuan, Xinhao Mei, Xubo Liu, Danilo Mandic, Wenwu Wang, Mark D. Plumbley*
    - *[PDF](https://arxiv.org/abs/2301.12503) [Code](https://audioldm.github.io/)*
- **‚ÄúMake-An-Audio: Text-To-Audio Generation with Prompt-Enhanced Diffusion Models‚Äù**
    - *Authors: Rongjie Huang, Jiawei Huang, Dongchao Yang, Yi Ren, Luping Liu, Mingze Li, Zhenhui Ye, Jinglin Liu, Xiang Yin, Zhou Zhao*
    - *[PDF](https://arxiv.org/abs/2301.12661)*
- **‚ÄúLarge-scale unsupervised audio pre-training for video-to-speech synthesis‚Äù**
    - *Authors: Triantafyllos Kefalas, Yannis Panagakis, Maja Pantic*
    - *[PDF](https://arxiv.org/abs/2306.15464)*
- **‚ÄúReVISE: Self-Supervised Speech Resynthesis With Visual Input for Universal and Generalized Speech Regeneration‚Äù**
    - *Authors: Wei-Ning Hsu, Tal Remez, Bowen Shi, Jacob Donley, Yossi Adi*
    - *[PDF](https://openaccess.thecvf.com/content/CVPR2023/papers/Hsu_ReVISE_Self-Supervised_Speech_Resynthesis_With_Visual_Input_for_Universal_and_CVPR_2023_paper.pdf)*
- **‚ÄúBack Translation for Speech-to-text Translation Without Transcripts‚Äù**
    - *Authors: Qingkai Fang, Yang Feng*
    - *[PDF](https://browse.arxiv.org/pdf/2305.08709.pdf)*
- ‚Äú**UniAudio: An Audio Foundation Model Toward Universal Audio Generation**‚Äù
    - *Authors: Dongchao Yang, Jinchuan Tian, Xu Tan, Rongjie Huang, Songxiang Liu, Xuankai Chang, Jiatong Shi, Sheng Zhao, Jiang Bian, Xixin Wu, Zhou Zhao, Helen Meng*
    - *[PDF](https://arxiv.org/abs/2310.00704)*
- **‚ÄúPromptTTS 2: Describing and Generating Voices with Text Prompt‚Äù**
    - *Authors: Yichong Leng, Zhifang Guo, Kai Shen, Xu Tan, Zeqian Ju, Yanqing Liu, Yufei Liu, Dongchao Yang, Leying Zhang, Kaitao Song, Lei He, Xiang-Yang Li, Sheng Zhao, Tao Qin, Jiang Bian*
    - *[PDF](https://arxiv.org/abs/2309.02285)*
- **‚ÄúVoicebox: Text-Guided Multilingual Universal Speech Generation at Scale‚Äù**
    - *Authors: Matthew Le, Apoorv Vyas, Bowen Shi, Brian Karrer, Leda Sari, Rashel Moritz, Mary Williamson, Vimal Manohar, Yossi Adi, Jay Mahadeokar, Wei-Ning Hsu*
    - *[PDF](https://arxiv.org/abs/2306.15687)*
- **‚ÄúStyleTTS 2: Towards Human-Level Text-to-Speech through Style Diffusion and Adversarial Training with Large Speech Language Models‚Äù**
    - *Authors: Yinghao Aaron Li, Cong Han, Vinay S. Raghavan, Gavin Mischler, Nima Mesgarani*
    - *[PDF](https://arxiv.org/abs/2306.07691)*

### üçª Cross-Modal Representation

Stay up to date on the latest advances in cross-modal representation learning:

- **‚ÄúCLAP Learning Audio Concepts from Natural Language Supervision‚Äù**
    - *Authors: Benjamin Elizalde, Soham Deshmukh, Mahmoud Al Ismail, Huaming Wang*
    - *[PDF](https://ieeexplore.ieee.org/abstract/document/10095889)*
- **‚ÄúSpeechCLIP: Integrating Speech with Pre-Trained Vision and Language Model‚Äù**
    - *Authors: Yi-Jen Shih, Hsuan-Fu Wang, Heng-Jui Chang, Layne Berry, Hung-yi Lee, David Harwath*
    - *[PDF](https://ieeexplore.ieee.org/abstract/document/10022954)*
- ‚Äú**BLSP: Bootstrapping Language-Speech Pre-training via Behavior Alignment of Continuation Writing**‚Äù
    - *Authors: Chen Wang, Minpeng Liao, Zhongqiang Huang, Jinliang Lu, Junhong Wu, Yuchen Liu, Chengqing Zong, Jiajun Zhang*
    - *[PDF](https://arxiv.org/abs/2309.00916)*
- ‚Äú**TalkCLIP: Talking Head Generation with Text-Guided Expressive Speaking Styles‚Äù**
    - *Authors:* Yifeng Ma, Suzhen Wang, Yu Ding, Bowen Ma, Tangjie Lv, Changjie Fan, Zhipeng Hu, Zhidong Deng, Xin Yu
    - *[PDF](https://arxiv.org/abs/2304.00334)*
- **‚ÄúVideo-LLaMA: An Instruction-tuned Audio-Visual Language Model for Video Understanding‚Äù**
    - *Authors: Hang Zhang,¬†Xin Li,¬†Lidong Bing*
    - *[PDF](https://arxiv.org/abs/2306.02858) [Code](https://github.com/DAMO-NLP-SG/Video-LLaMA)*
- **‚ÄúMusic Understanding LLaMA: Advancing Text-to-Music Generation with Question Answering and Captioning‚Äù**
    - *Authors: Shansong Liu,¬†Atin Sakkeer Hussain,¬†Chenshuo Sun,¬†Ying Shan*
    - *[PDF](https://arxiv.org/abs/2308.11276)*
- **‚ÄúEfficient Self-supervised Learning with Contextualized Target Representations
for Vision, Speech and Language‚Äù**
    - *Authors: Alexei Baevski, Arun Babu, Wei-Ning Hsu, Michael Auli*
    - *[PDF](https://proceedings.mlr.press/v202/baevski23a/baevski23a.pdf)*
- **‚ÄúModality Adaption or Regularization? A Case Study on End-to-End Speech Translation‚Äù**
    - *Authors: Yuchen Han, Chen Xu, Tong Xiao, Jingbo Zhu*
    - *[PDF](https://arxiv.org/abs/2306.07650)  [Code](https://github.com/hannlp/TAB)*
- **‚ÄúMERT: Acoustic Music Understanding Model with Large-Scale Self-supervised Training‚Äù**
    - *Authors: Yizhi Li, Ruibin Yuan, Ge Zhang, Yinghao Ma, Xingran Chen, Hanzhi Yin, Chenghua Lin, Anton Ragni, Emmanouil Benetos, Norbert Gyenge, Roger Dannenberg, Ruibo Liu, Wenhu Chen, Gus Xia, Yemin Shi, Wenhao Huang, Yike Guo, Jie Fu*
    - *[PDF](https://arxiv.org/abs/2306.00107)  [Code](https://github.com/yizhilll/MERT)*
- **‚ÄúVAST: A Vision-Audio-Subtitle-Text Omni-Modality Foundation Model and Dataset‚Äù**
    - *Authors: Sihan Chen, Handong Li, Qunbo Wang, Zijia Zhao, Mingzhen Sun, Xinxin Zhu, Jing Liu*
    - *[PDF](https://arxiv.org/abs/2305.18500)  [Code](https://github.com/TXH-mercury/VAST)*
- **‚ÄúX-LLM: Bootstrapping Advanced Large Language Models by Treating Multi-Modalities as Foreign Languages‚Äù**
    - *Authors: Yuchen Han, Chen Xu, Tong Xiao, Jingbo Zhu*
    - *[PDF](https://arxiv.org/abs/2306.07650)  [Code](https://x-llm.github.io/)*
- **‚ÄúALOR: Vision-Audio-Language Omni-Perception Pretraining Model and Dataset‚Äù**
    - *Authors:* Sihan Chen, Xingjian He, Longteng Guo, Xinxin Zhu, Weining Wang, Jinhui Tang, *Jing Liu*
    - *[PDF](https://arxiv.org/abs/2304.08345)  [Code](https://casia-iva-group.github.io/projects/VALOR)*

### üìà Benchmarks

Track the progress of large audio models through benchmark performance metrics:

- **ASR Leaderboard**: Monitor the latest results in automatic speech recognition (ASR) across diverse datasets, evaluating the accuracy, speed, and memory efficiency of various models.
- **Speech Synthesis Quality**: Explore benchmark scores for evaluating the quality and naturalness of synthesized speech, ensuring your applications deliver a premium auditory experience.
- **Audio Classification Challenges**: Stay up-to-date with ongoing audio classification challenges and assess model performance across different domains, from environmental sounds to musical genres.

### üåê Community Contributions

We encourage the audio research community to actively contribute to this repository by adding relevant papers, benchmark results, or tools that can benefit the community at large. Let's collaboratively advance the field of large audio models!

### üôå Acknowledgments

We'd like to express our gratitude to all the researchers and developers who tirelessly work on improving audio models and sharing their findings with the community. This repository stands as a testament to your dedication.

### üìÑ Citation

If you find this resource helpful in your research, please consider citing us:

```
@github{Awesome-Large-Audio-Models,
  title = {Awesome-Large-Audio-Models},
  author = {Your Name},
  year = {2023},
  url = {<https://github.com/yourusername/Awesome-Large-Audio-Models>},
  note = {Your specific note (optional)}
}

```

### üåü Let's Elevate Audio Research Together!

Join us in the quest to push the boundaries of large audio models. Together, we can create remarkable advancements in audio processing and understanding. Explore, contribute, and innovate! üöÄüéßüîä
